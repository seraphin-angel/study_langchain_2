{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クックブック: Langchain 統合\n",
    "https://langfuse.com/docs/integrations/langchain/example-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    " \n",
    "langfuse_handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests the SDK connection with the server\n",
    "langfuse_handler.auth_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain 式言語 (LCEL) におけるシーケンシャル チェーン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'バラク・オバマがしばしば関連付けられる都市は、アメリカ合衆国のイリノイ州シカゴです。彼は1980年代にそこに移り住み、州上院議員として、そして後にアメリカ合衆国上院議員として政治キャリアを始め、最終的には大統領になりました。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    " \n",
    "langfuse_handler = CallbackHandler(\n",
    "    trace_name=\"langchain-example-20240825-001\",\n",
    "    session_id=\"user-1234\",\n",
    "    user_id=\"session-1234\",\n",
    ")\n",
    " \n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    " \n",
    "chain2.invoke({\"person\": \"obama\", \"language\": \"japanese\"}, config={\"callbacks\":[langfuse_handler]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Die Stadt Wilmington, in der Joe Biden lebt, befindet sich in den Vereinigten Staaten von Amerika.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Async Invoke\n",
    "await chain2.ainvoke({\"person\": \"biden\", \"language\": \"german\"}, config={\"callbacks\":[langfuse_handler]})\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Die Stadt, in der Elon Musk geboren wurde, Pretoria, liegt in Südafrika.',\n",
       " 'マーク・ザッカーバーグはアメリカ合衆国のニューヨーク州のホワイトプレインズ出身です。彼は近くのダブズ・フェリーで育ちました。']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch\n",
    "chain2.batch([{\"person\": \"elon musk\", \"language\": \"german\"}, {\"person\": \"mark zuckerberg\", \"language\": \"japanese\"}], config={\"callbacks\":[langfuse_handler]})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Die Stadt Albuquerque, New Mexico, befindet sich in den Vereinigten Staaten.',\n",
       " 'ティム・クックはアメリカ合衆国のモバイル、アラバマ出身です。']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Async Batch\n",
    "await chain2.abatch([{\"person\": \"jeff bezos\", \"language\": \"german\"}, {\"person\": \"tim cook\", \"language\": \"japanese\"}], config={\"callbacks\":[langfuse_handler]})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming chunk: \n",
      "Streaming chunk: Steve\n",
      "Streaming chunk:  Jobs\n",
      "Streaming chunk:  was\n",
      "Streaming chunk:  born\n",
      "Streaming chunk:  in\n",
      "Streaming chunk:  the\n",
      "Streaming chunk:  United\n",
      "Streaming chunk:  States\n",
      "Streaming chunk: ,\n",
      "Streaming chunk:  specifically\n",
      "Streaming chunk:  in\n",
      "Streaming chunk:  San\n",
      "Streaming chunk:  Francisco\n",
      "Streaming chunk: ,\n",
      "Streaming chunk:  California\n",
      "Streaming chunk: .\n",
      "Streaming chunk:  He\n",
      "Streaming chunk:  grew\n",
      "Streaming chunk:  up\n",
      "Streaming chunk:  in\n",
      "Streaming chunk:  Cupertino\n",
      "Streaming chunk: ,\n",
      "Streaming chunk:  California\n",
      "Streaming chunk: ,\n",
      "Streaming chunk:  which\n",
      "Streaming chunk:  is\n",
      "Streaming chunk:  also\n",
      "Streaming chunk:  in\n",
      "Streaming chunk:  the\n",
      "Streaming chunk:  United\n",
      "Streaming chunk:  States\n",
      "Streaming chunk: .\n",
      "Streaming chunk: \n"
     ]
    }
   ],
   "source": [
    "# Stream\n",
    "for chunk in chain2.stream({\"person\": \"steve jobs\", \"language\": \"english\"}, config={\"callbacks\":[langfuse_handler]}):\n",
    "    print(\"Streaming chunk:\", chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Async Streaming chunk: \n",
      "Async Streaming chunk: Seattle\n",
      "Async Streaming chunk: ,\n",
      "Async Streaming chunk:  Washington\n",
      "Async Streaming chunk: ,\n",
      "Async Streaming chunk:  is\n",
      "Async Streaming chunk:  in\n",
      "Async Streaming chunk:  the\n",
      "Async Streaming chunk:  United\n",
      "Async Streaming chunk:  States\n",
      "Async Streaming chunk: .\n",
      "Async Streaming chunk: \n"
     ]
    }
   ],
   "source": [
    "# Async Stream\n",
    "async for chunk in chain2.astream({\"person\": \"bill gates\", \"language\": \"english\"}, config={\"callbacks\":[langfuse_handler]}):\n",
    "    print(\"Async Streaming chunk:\", chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 会話チェーン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    " \n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    " \n",
    "conversation = ConversationChain(\n",
    "    llm=llm, memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! To build great developer tools, focus on understanding your audience and their needs, clearly define the problem your tool solves, and prioritize user-centric design for a good UI/UX. Ensure robust functionality, performance, and reliability, and provide comprehensive documentation and support. Gather user feedback for continuous improvement, promote your tool within developer communities, and consider open-sourcing it for collaboration. Lastly, stay updated on tech trends to keep your tool relevant. Do you have a specific type of tool in mind?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a callback handler with a session\n",
    "langfuse_handler = CallbackHandler(session_id=\"conversation_chain\")\n",
    "\n",
    "conversation.predict(input=\"Hi there!\", callbacks=[langfuse_handler])\n",
    "conversation.predict(input=\"How to build great developer tools?\", callbacks=[langfuse_handler])\n",
    "conversation.predict(input=\"Summarize your last response\", callbacks=[langfuse_handler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検索QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What did the president say about Ketanji Brown Jackson',\n",
       " 'result': \"The president described Ketanji Brown Jackson as one of the nation's top legal minds and stated that she will continue Justice Breyer's legacy of excellence.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import SeleniumURLLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    " \n",
    "langfuse_handler = CallbackHandler()\n",
    " \n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/langfuse/langfuse-docs/main/public/state_of_the_union.txt\",\n",
    "]\n",
    "loader = SeleniumURLLoader(urls=urls)\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=docsearch.as_retriever(search_kwargs={\"k\": 1}),\n",
    ")\n",
    " \n",
    "chain.invoke(query, config={\"callbacks\":[langfuse_handler]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エージェント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AzureオープンAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### シーケンシャルチェーン [レガシー]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トレースにスコアを追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import langfuse_context, observe\n",
    " \n",
    "@observe(as_type=\"generation\")\n",
    "def deeply_nested_llm_call():\n",
    "    # Enrich the current observation with a custom name, input, and output\n",
    "    langfuse_context.update_current_observation(\n",
    "        name=\"Deeply nested LLM call222\", input=\"Ping?\", output=\"Pong!\"\n",
    "    )\n",
    "    # Set the parent trace's name from within a nested observation\n",
    "    langfuse_context.update_current_trace(\n",
    "        name=\"Trace name set from deeply_nested_llm_call\",\n",
    "        session_id=\"1234\",\n",
    "        user_id=\"5678\",\n",
    "        tags=[\"tag1\", \"tag2\"],\n",
    "        public=True\n",
    "    )\n",
    " \n",
    "@observe()\n",
    "def nested_span():\n",
    "    # Update the current span with a custom name and level\n",
    "    langfuse_context.update_current_observation(name=\"Nested Span\", level=\"WARNING\")\n",
    "    deeply_nested_llm_call()\n",
    " \n",
    "@observe()\n",
    "def main():\n",
    "    nested_span()\n",
    " \n",
    "# Execute the main function to generate the enriched trace\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langfuse Python SDK との相互運用性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import langfuse_context, observe\n",
    " \n",
    "# Create a trace via Langfuse decorators and get a Langchain Callback handler for it\n",
    "@observe() # automtically log function as a trace to Langfuse\n",
    "def main():\n",
    "    # update trace attributes (e.g, name, session_id, user_id)\n",
    "    langfuse_context.update_current_trace(\n",
    "        name=\"custom-trace\",\n",
    "        session_id=\"user-1234\",\n",
    "        user_id=\"session-1234\",\n",
    "    )\n",
    "    # get the langchain handler for the current trace\n",
    "    langfuse_context.get_current_langchain_handler()\n",
    " \n",
    "    # use the handler to trace langchain runs ...\n",
    " \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    " \n",
    "prompt = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    " \n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import langfuse_context, observe\n",
    " \n",
    "# On span \"Physics\".\"Favorites\"\n",
    "@observe()  # decorator to automatically log function as sub-span to Langfuse\n",
    "def favorites():\n",
    "    # get the langchain handler for the current sub-span\n",
    "    langfuse_handler = langfuse_context.get_current_langchain_handler()\n",
    "    # invoke chain with langfuse handler\n",
    "    chain.invoke({\"person\": \"Richard Feynman\"},\n",
    "                 config={\"callbacks\": [langfuse_handler]})\n",
    " \n",
    "# On span \"Physics\"\n",
    "@observe()  # decorator to automatically log function as span to Langfuse\n",
    "def physics():\n",
    "    # get the langchain handler for the current span\n",
    "    langfuse_handler = langfuse_context.get_current_langchain_handler()\n",
    "    # invoke chains with langfuse handler\n",
    "    chain.invoke({\"person\": \"Albert Einstein\"},\n",
    "                 config={\"callbacks\": [langfuse_handler]})\n",
    "    chain.invoke({\"person\": \"Isaac Newton\"},\n",
    "                 config={\"callbacks\": [langfuse_handler]})\n",
    "    favorites()\n",
    " \n",
    "# On trace\n",
    "@observe()  # decorator to automatically log function as trace to Langfuse\n",
    "def main():\n",
    "    # get the langchain handler for the current trace\n",
    "    langfuse_handler = langfuse_context.get_current_langchain_handler()\n",
    "    # invoke chain with langfuse handler\n",
    "    chain.invoke({\"person\": \"Alan Turing\"},\n",
    "                 config={\"callbacks\": [langfuse_handler]})\n",
    "    physics()\n",
    " \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import langfuse_context, observe\n",
    " \n",
    "# On span \"Physics\".\"Favorites\"\n",
    "# @observe()  # decorator to automatically log function as sub-span to Langfuse\n",
    "def favorites():\n",
    "    # get the langchain handler for the current sub-span\n",
    "    langfuse_handler = langfuse_context.get_current_langchain_handler()\n",
    "    # invoke chain with langfuse handler\n",
    "    chain.invoke({\"person\": \"Richard Feynman\"},\n",
    "                 config={\"callbacks\": [langfuse_handler]})\n",
    " \n",
    "# On span \"Physics\"\n",
    "# @observe()  # decorator to automatically log function as span to Langfuse\n",
    "def physics():\n",
    "    # get the langchain handler for the current span\n",
    "    langfuse_handler = langfuse_context.get_current_langchain_handler()\n",
    "    # invoke chains with langfuse handler\n",
    "    chain.invoke({\"person\": \"Albert Einstein\"},\n",
    "                 config={\"callbacks\": [langfuse_handler]})\n",
    "    chain.invoke({\"person\": \"Isaac Newton\"},\n",
    "                 config={\"callbacks\": [langfuse_handler]})\n",
    "    favorites()\n",
    " \n",
    "# On trace\n",
    "@observe()  # decorator to automatically log function as trace to Langfuse\n",
    "def main():\n",
    "    # get the langchain handler for the current trace\n",
    "    langfuse_handler = langfuse_context.get_current_langchain_handler()\n",
    "    # invoke chain with langfuse handler\n",
    "    chain.invoke({\"person\": \"Alan Turing\"},\n",
    "                 config={\"callbacks\": [langfuse_handler]})\n",
    "    physics()\n",
    " \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
