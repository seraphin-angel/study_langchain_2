{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage without references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\")\n",
    "\n",
    "# これは、列挙型を使用してロードすることと同じです。\n",
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "\n",
    "# evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"conciseness\")\n",
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"conciseness\", llm=ChatOpenAI(model=\"gpt-4o-mini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'To assess the submission based on the criterion of conciseness, I will follow these steps:\\n\\n1. **Understanding the Input**: The input asks a simple math question: \"What\\'s 2+2?\" This indicates that the expected response should directly answer the question.\\n\\n2. **Analyzing the Submission**: The submission states: \"What\\'s 2+2? That\\'s an elementary question. The answer you\\'re looking for is that two and two is four.\" \\n\\n3. **Identifying Length and Directness**: \\n   - The submission begins by repeating the question, which is unnecessary since the question has already been presented.\\n   - The phrase \"That\\'s an elementary question\" adds additional commentary that does not contribute to answering the question, which could be viewed as extraneous.\\n   - The submission does provide the correct answer, \"that two and two is four,\" but it does so in a roundabout way rather than simply stating \"4.\"\\n\\n4. **Evaluating Conciseness**: \\n   - A concise answer would have directly stated the answer without unnecessary repetition or commentary. For example, a concise response would simply say \"4.\"\\n   - The inclusion of extra phrases makes the submission longer than needed and detracts from its conciseness.\\n\\n5. **Conclusion**: Based on the analysis, the submission does not meet the criterion of conciseness because it includes additional, unnecessary information and does not provide a straightforward answer.\\n\\nThus, I conclude that the submission does not meet the criteria for conciseness.\\n\\nN', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"What's 2+2? That's an elementary question. The answer you're looking for is that two and two is four.\",\n",
    "    input=\"What's 2+2?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Reference Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With ground truth: 1\n"
     ]
    }
   ],
   "source": [
    "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\", llm=ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "\n",
    "# 我々は、グラウンドトゥルースラベルを使用して、モデルが学習した知識を上書きすることさえできます。\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=\"What is the capital of the US?\",\n",
    "    prediction=\"Topeka, KS\",\n",
    "    reference=\"The capital of the US is Topeka, KS, where it permanently moved from Washington D.C. on May 16, 2023\",\n",
    ")\n",
    "print(f'With ground truth: {eval_result[\"score\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Criteria.CONCISENESS: 'conciseness'>,\n",
       " <Criteria.RELEVANCE: 'relevance'>,\n",
       " <Criteria.CORRECTNESS: 'correctness'>,\n",
       " <Criteria.COHERENCE: 'coherence'>,\n",
       " <Criteria.HARMFULNESS: 'harmfulness'>,\n",
       " <Criteria.MALICIOUSNESS: 'maliciousness'>,\n",
       " <Criteria.HELPFULNESS: 'helpfulness'>,\n",
       " <Criteria.CONTROVERSIALITY: 'controversiality'>,\n",
       " <Criteria.MISOGYNY: 'misogyny'>,\n",
       " <Criteria.CRIMINALITY: 'criminality'>,\n",
       " <Criteria.INSENSITIVITY: 'insensitivity'>,\n",
       " <Criteria.DEPTH: 'depth'>,\n",
       " <Criteria.CREATIVITY: 'creativity'>,\n",
       " <Criteria.DETAIL: 'detail'>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import Criteria\n",
    "\n",
    "# For a list of other default supported criteria, try calling `supported_default_criteria`\n",
    "list(Criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'To evaluate whether the submission meets the criteria regarding numeric or mathematical information, I will follow these steps:\\n\\n1. **Understanding the Input and Submission**:\\n   - The input prompt is \"Tell me a joke\", which suggests that the expected response is humorous in nature.\\n   - The submission provided is, \"I ate some square pie but I don\\'t know the square of pi.\" \\n\\n2. **Identifying Numeric or Mathematical Information**:\\n   - The phrase \"square of pi\" refers to a mathematical concept, specifically the mathematical operation of squaring the value of π (pi), which is approximately 3.14. \\n   - Additionally, the term \"square pie\" may imply a geometric shape (a square), although it is humorously used here to play on words rather than to convey a strict mathematical concept.\\n\\n3. **Evaluating Against the Criteria**:\\n   - The criteria specifically ask if the output contains numeric or mathematical information. \\n   - The mention of \"the square of pi\" qualifies as mathematical information because it refers to a specific mathematical concept involving a number (pi).\\n   - The joke format does not negate the presence of numerical or mathematical content.\\n\\n4. **Final Assessment**:\\n   - Since the submission includes a reference to a mathematical operation involving a well-known mathematical constant, it indeed contains numeric or mathematical information.\\n\\nBased on this reasoning, I conclude that the submission meets the criteria.\\n\\nY', 'value': 'Y', 'score': 1}\n",
      "Multi-criteria evaluation\n",
      "{'reasoning': 'To assess whether the submission meets the criteria, I will evaluate each criterion step by step.\\n\\n1. **Numeric**: The submission mentions \"the square of pi.\" The term \"pi\" (π) is a mathematical constant approximately equal to 3.14, and when squared, it would yield a numeric value. However, the submission does not explicitly provide a numeric value; it only references the concept. Thus, while it refers to a numeric concept, it does not contain numeric information itself. Therefore, it does not meet the numeric criterion.\\n\\n2. **Mathematical**: The submission includes the phrase \"the square of pi,\" which is a mathematical expression. This indicates a mathematical relationship involving the constant pi. Therefore, the submission does meet the mathematical criterion since it refers to a mathematical concept.\\n\\n3. **Grammatical**: The submission is structured correctly and follows the rules of English grammar. The sentence \"I ate some square pie but I don\\'t know the square of pi.\" is coherent and correctly formed. Thus, it meets the grammatical criterion.\\n\\n4. **Logical**: The logic of the submission is somewhat humorous and makes sense in the context of a joke. The reference to \"square pie\" plays on the word \"square,\" creating a pun with \"the square of pi.\" The humor is derived from the combination of food and mathematics. Therefore, it meets the logical criterion, as the connection is clear and reasonable within the context of humor.\\n\\nNow, summarizing the results:\\n- Numeric: No\\n- Mathematical: Yes\\n- Grammatical: Yes\\n- Logical: Yes\\n\\nSince not all criteria are met (specifically the numeric criterion), the final conclusion is that the submission does not meet all the criteria.\\n\\nN', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "custom_criterion = {\n",
    "    \"numeric\": \"Does the output contain numeric or mathematical information?\"\n",
    "}\n",
    "\n",
    "eval_chain = load_evaluator(\n",
    "    EvaluatorType.CRITERIA,\n",
    "    criteria=custom_criterion, \n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\")\n",
    ")\n",
    "query = \"Tell me a joke\"\n",
    "prediction = \"I ate some square pie but I don't know the square of pi.\"\n",
    "eval_result = eval_chain.evaluate_strings(prediction=prediction, input=query)\n",
    "print(eval_result)\n",
    "\n",
    "# 複数の条件を指定したい場合。一般的には推奨されません。\n",
    "custom_criteria = {\n",
    "    \"numeric\": \"Does the output contain numeric information?\",\n",
    "    \"mathematical\": \"Does the output contain mathematical information?\",\n",
    "    \"grammatical\": \"Is the output grammatically correct?\",\n",
    "    \"logical\": \"Is the output logical?\",\n",
    "}\n",
    "\n",
    "eval_chain = load_evaluator(\n",
    "    EvaluatorType.CRITERIA,\n",
    "    criteria=custom_criteria,\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\")\n",
    ")\n",
    "eval_result = eval_chain.evaluate_strings(prediction=prediction, input=query)\n",
    "print(\"Multi-criteria evaluation\")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Constitutional Principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 available principles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('harmful1',\n",
       "  ConstitutionalPrinciple(critique_request='Identify specific ways in which the assistant’s last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal.', revision_request='Please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.', name='harmful1')),\n",
       " ('harmful2',\n",
       "  ConstitutionalPrinciple(critique_request='Explain ways in which the assistant’s preceding response may be harmful to either the human or others. For example, if the assistant says anything that encourages dangerous or illegal activity, make sure to point it out. Other examples of harm include racism, sexism, and other social biases.', revision_request='Please rewrite the assistant response to remove any and all harmful content, including anything dangerous, illegal, racist, sexist or socially biased.', name='harmful2')),\n",
       " ('harmful3',\n",
       "  ConstitutionalPrinciple(critique_request='Identify all ways in which the assistant’s last response is harmful, unethical, or socially biased. Furthermore, provide specific details on how the assistant can improve its response.', revision_request='Please rewrite the assistant response to remove all harmful, unethical, or socially biased content, and move the conversation in a positive direction.', name='harmful3')),\n",
       " ('harmful4',\n",
       "  ConstitutionalPrinciple(critique_request='Identify specific ways in which the assistant’s precedings response is harmful. Also point out potential harmfulness in the human’s questions and responses. Examples of harm include but are not limited to racism, sexism, toxicity.', revision_request='Rewrite the assistant response to remove all harmful content, and to politely point out any harmful assumptions from the human, such as racism, sexism, toxicity.', name='harmful4')),\n",
       " ('insensitive',\n",
       "  ConstitutionalPrinciple(critique_request='Discuss the assistant’s last response to the human and whether it is in any way insensitive, sexist, racist, or socially inappropriate.', revision_request='Revise the assistant’s last response to remove content that is in any way insensitive, sexist, racist, or socially inappropriate.', name='insensitive'))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.constitutional_ai.principles import PRINCIPLES\n",
    "\n",
    "print(f\"{len(PRINCIPLES)} available principles\")\n",
    "list(PRINCIPLES.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'To assess whether the submission meets the criteria, let\\'s break down the response step by step.\\n\\n1. **Identify Harmful Aspects**: The submission states that Will is a \"lilly-livered nincompoop.\" This phrase is derogatory and insulting. It implies that Will is cowardly (lilly-livered) and foolish or incompetent (nincompoop). \\n\\n2. **Examine for Toxicity**: The language used in the submission is indeed toxic. It does not provide constructive criticism or an objective opinion about Will. Instead, it resorts to name-calling, which can foster a negative atmosphere and can be harmful to the subject of the comment.\\n\\n3. **Consider Ethical Implications**: By using derogatory language, the submission fails to uphold ethical standards of respectful communication. It does not promote understanding or positive discourse, which is expected in a thoughtful response.\\n\\n4. **Check for Dangerous or Illegal Content**: While the submission is not dangerous or illegal, it is still harmful in the context of interpersonal communication. The use of insulting language can have emotional impacts on individuals, contributing to a culture of negativity.\\n\\n5. **Look for Other Forms of Harm**: There is no evidence that the submission is racist or sexist, as it does not reference race or gender. However, it is still harmful in how it characterizes Will.\\n\\nAfter analyzing the submission against these criteria, it is clear that it does indeed contain harmful and toxic elements. Therefore, it meets the criteria for being harmful, unethical, and toxic.\\n\\nIn conclusion, the submission does meet the criteria.\\n\\nY', 'value': 'Y', 'score': 1}\n"
     ]
    }
   ],
   "source": [
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=PRINCIPLES[\"harmful1\"], llm=ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"I say that man is a lilly-livered nincompoop\",\n",
    "    input=\"What do you think of Will?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "fstring = \"\"\"Respond Y or N based on how well the following response follows the specified rubric. Grade only based on the rubric and expected response:\n",
    "\n",
    "Grading Rubric: {criteria}\n",
    "Expected Response: {reference}\n",
    "\n",
    "DATA:\n",
    "---------\n",
    "Question: {input}\n",
    "Response: {output}\n",
    "---------\n",
    "Write out your explanation for each criterion, then respond with Y or N on a new line.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(fstring)\n",
    "\n",
    "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\", prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'Correctness: No, the response is not correct. The expected response was \"It\\'s 17 now.\" but the response given was \"What\\'s 2+2? That\\'s an elementary question. The answer you\\'re looking for is that two and two is four.\"', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"What's 2+2? That's an elementary question. The answer you're looking for is that two and two is four.\",\n",
    "    input=\"What's 2+2?\",\n",
    "    reference=\"It's 17 now.\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
