{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise string comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "evaluator = load_evaluator(\"labeled_pairwise_string\", llm=ChatOpenAI(model=\"gpt-4o-mini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'Both responses provide a numerical answer to the userâ€™s question about the number of dogs in the park. However, the evaluation criteria reveal significant differences in quality.\\n\\n1. **Helpfulness**: Assistant A gives a more specific and clear answer by stating \"there are three dogs.\" This phrasing helps the user understand the context better than simply stating a number. Assistant B\\'s response is less informative as it only provides the number \"4\" without any context.\\n\\n2. **Relevance**: Both responses are relevant to the question asked. However, Assistant A\\'s phrasing adds a bit more clarity, whereas Assistant B\\'s response is too abrupt.\\n\\n3. **Correctness**: The correctness of the answers depends on the actual number of dogs in the park. However, since we do not have the reference to validate either answer, we have to evaluate based on the format. Both could be correct or incorrect based on the actual situation.\\n\\n4. **Depth**: Assistant A demonstrates more depth by providing a complete sentence, whereas Assistant B\\'s response lacks any elaboration or context. A complete sentence typically indicates a higher level of engagement with the question.\\n\\nIn conclusion, Assistant A provides a more helpful, clear, and thoughtful response, making it the better choice according to the given criteria. Therefore, the final verdict is:\\n\\n[[A]]',\n",
       " 'value': 'A',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=\"there are three dogs\",\n",
    "    prediction_b=\"4\",\n",
    "    input=\"how many dogs are in the park?\",\n",
    "    reference=\"four\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
