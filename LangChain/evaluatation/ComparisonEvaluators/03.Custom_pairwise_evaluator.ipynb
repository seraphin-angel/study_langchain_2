{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom pairwise evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "from langchain.evaluation import PairwiseStringEvaluator\n",
    "\n",
    "\n",
    "class LengthComparisonPairwiseEvaluator(PairwiseStringEvaluator):\n",
    "    \"\"\"\n",
    "    Custom evaluator to compare two strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def _evaluate_string_pairs(\n",
    "        self,\n",
    "        *,\n",
    "        prediction: str,\n",
    "        prediction_b: str,\n",
    "        reference: Optional[str] = None,\n",
    "        input: Optional[str] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> dict:\n",
    "        score = int(len(prediction.split()) > len(prediction_b.split()))\n",
    "        return {\"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = LengthComparisonPairwiseEvaluator()\n",
    "\n",
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=\"The quick brown fox jumped over the lazy dog dog.\",\n",
    "    prediction_b=\"The quick brown fox jumped over the dog.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-Based Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3713648768.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\"Which option is preferred? Do not take order into account. Evaluate based on accuracy and helpfulness. If neither is preferred, respond with C. Provide your reasoning, then finish with Preference: A/B/C\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.evaluation import PairwiseStringEvaluator\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class CustomPreferenceEvaluator(PairwiseStringEvaluator):\n",
    "    \"\"\"\n",
    "    Custom evaluator to compare two strings using a custom LLMChain.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        self.eval_chain = LLMChain.from_string(\n",
    "            \"\"\"Which option is preferred? Do not take order into account. Evaluate based on accuracy and helpfulness. If neither is preferred, respond with C. Provide your reasoning, then finish with Preference: A/B/C\n",
    "\n",
    "Input: How do I get the path of the parent directory in python 3.8?\n",
    "Option A: You can use the following code:\n",
    "```python\n",
    "import os\n",
    "\n",
    "os.path.dirname(os.path.dirname(os.path.abspath(__file__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 2\u001b[0m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mabsolute()\u001b[38;5;241m.\u001b[39mparent\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "Path(__file__).absolute().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def requires_input(self) -> bool:\n",
    "    return True\n",
    "\n",
    "@property\n",
    "def requires_reference(self) -> bool:\n",
    "    return False\n",
    "\n",
    "def _evaluate_string_pairs(\n",
    "    self,\n",
    "    *,\n",
    "    prediction: str,\n",
    "    prediction_b: str,\n",
    "    reference: Optional[str] = None,\n",
    "    input: Optional[str] = None,\n",
    "    **kwargs: Any,\n",
    ") -> dict:\n",
    "    result = self.eval_chain(\n",
    "        {\n",
    "            \"input\": input,\n",
    "            \"prediction\": prediction,\n",
    "            \"prediction_b\": prediction_b,\n",
    "            \"stop\": [\"Which option is preferred?\"],\n",
    "        },\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    response_text = result[\"text\"]\n",
    "    reasoning, preference = response_text.split(\"Preference:\", maxsplit=1)\n",
    "    preference = preference.strip()\n",
    "    score = 1.0 if preference == \"A\" else (0.0 if preference == \"B\" else None)\n",
    "    return {\"reasoning\": reasoning.strip(), \"value\": preference, \"score\": score}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
